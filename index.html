<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hermann Kumbong</title>

    <meta name="author" content="Hermann Kumbong">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hermann Kumbong
                </p>
                <p>I am a Masters in Computer Science student at Stanford University working at the intersection of Machine Learning and Systems. I am supported by the <a href="https://wadescholarship.org/program/">Wade Scholarship</a> and I am fortunate to be advised by <a href="https://cs.stanford.edu/~chrismre/">Prof. Chris R&eacute;</a>.
                </p>
                <p>
                  Before Stanford, I spent 2 amazing years at Goldman Sachs London, building low-latency trading systems. I completed my BSc. in Computer Engineering from the <a href="https://www.knust.edu.gh/">Kwame Nkrumah University of Science and Technology</a>, where I was supported by the <a href="https://mastercardfdn.org/all/scholars/">Mastercard Foundation Scholarship</a> and graduated as the <a href="https://www.youtube.com/watch?v=HMekq8mLYCM&t=11s&ab_channel=KNUSTLiveTV">University Valedictorian</a>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:kumboh@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=NnL2qHgAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
				          <a href="https://www.linkedin.com/in/hermannkumbong/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/KumbongHermann">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Kumbong">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile_pic.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile_pic.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in efficient systems for machine learning workloads and in applying machine learning to designing/optimizing computer systems.
                </p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/flashfftconv.png" alt="Modal Interpolation" width="160" height="100">
              </td> -->
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2311.05908.pdf">
                  <span class="papertitle">FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores</span>
                </a>
                <br>
                <a href="https://www.danfu.org/">Daniel Y Fu*</a>,
                <strong>Hermann Kumbong*</strong>,
                <a href="http://erictnguyen.com/">Eric Nguyen</a>,
                <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>
                <br>
                <em>Under Review</em>, 2023
                <br>
                <em>Third Workshop on Efficient Natural Language and Speech Processing @ NEURIPS</em>, 2023 <font color="red">(Oral)</font> 
                <!-- <p>FlashFFTConv speeds up exact FFT convolutions by up to 7.93x over PyTorch, reduces the memory footprint, and achieves 4.4x speedup end-to-end!!</p> -->
              </td>
            </tr>

            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/hedgehog.png" alt="HedgeHog" width="160" height="100">
              </td> -->
              <td width="100%" valign="middle">
                <a href="https://neurips2023-enlsp.github.io/accepted_papers.html">
                  <span class="papertitle">Improving Linear Attention via Softmax Mimicry</span>
                </a>
                <br>
                <a href="https://michaelzhang.xyz/">Michael Zhang</a>,
                Kush Bhatia,
                <strong>Hermann Kumbong</strong>,
                <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>
                <br>
                <em>Under Review</em>, 2023
                <br>
                <em>Third Workshop on Efficient Natural Language and Speech Processing @ NEURIPS</em>, 2023 <font color="red">(Oral)</font> 
                <!-- <p>We introduce Hedgehog, a learnable linear attention trained to “mimic” softmax attention by minimizing cross-entropy between
                  attention weights. Experiments show Hedgehog significantly closes the attention
                  performance gap between standard and linear attention. </p> -->
              </td>
            </tr>

            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/hyena.png" alt="Modal Interpolation" width="160" height="100">
              </td> -->
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2310.18780.pdf">
                  <span class="papertitle">Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions</span>
                </a>
                <br>
                <a href="https://massaroli.site/">Stefano Massaroli*</a>, <a href="https://zymrael.github.io/"> Michael Poli*</a>, <a href="https://www.danfu.org/">Daniel Y Fu*</a>, <strong>Hermann Kumbong</strong>, <a href="https://github.com/ruke1ire">Rom Nishijima Parnichkun</a>, <a href="https://www.davidromero.ml/"> David W. Romero</a>, 
                <a href="https://amantimalsina.github.io/">Aman Timalsina</a>,
                <a href="https://www.linkedin.com/in/quinn-mcintyre/">Quinn McIntyre</a>,
                <a href="https://www.andrew.cmu.edu/user/beidic/">Beidi Chen</a>,
                <a href="https://cse.buffalo.edu/faculty/atri/">Atri Rudra</a>,
                <a href="https://zhangce.github.io/">Ce Zhang</a>,
                <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>,
                <a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>,
                <a href="https://yoshuabengio.org/">Yoshua Bengio</a>
                <br>
                <em>Advances in Neural Information Processing Systems (NEURIPS)</em>, 2023
                <!-- <p>We refine long convolution sequence models in quality and efficiency, developing methods to distill compact recurrences from large convolutional models without loss in quality, and introduce architectural improvements that lead to increased throughput.</p> -->
              </td>
            </tr>


            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gptzip.png" alt="clean-usnob" width="160" height="100">
              </td> -->
              <td width="100%" valign="middle">
                <a href="https://openreview.net/pdf?id=hO0c2tG2xL">
                  <span class="papertitle">GPT-Zip: Deep Compression of Finetuned Large Language Models</span>
                </a>
                <br>
                <a href="https://sites.google.com/view/berivanisik">Berivan Isik*</a>,
                <strong>Hermann Kumbong*</strong>,
                Wanyi Ning*,
                <a href="https://xzyaoi.github.io/">Xiaozhe Yao*</a>,
                <a href="https://cs.stanford.edu/~sanmi/">Sanmi Koyejo</a>,
                <a href="https://zhangce.github.io/">Ce Zhang</a>
                <br>
                <em>ICML Efficient Systems for Foundation Models Workshop</em>, 2023
                <!-- <p>GPT-Zip is a post-finetuning compression technique that uses quantization and sparsification to
                  efficiently compress finetuned models by exploiting their closeness to the pretrained base model</p> -->
              </td>
            </tr>

            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/antenna.png" alt="clean-usnob" width="160" height="100">
              </td> -->
              <td width="100%" valign="middle">
                <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/eng2.12407">
                  <span class="papertitle">On the design and implementation of efficient antennas for high frequency‐radio frequency identification read/write devices</span>
                </a>
                <br>
                Ernest Ofosu Addo,
                Benjamin Kommey,
                Andrew Selasi Agbemenu,
                <strong>Hermann Kumbong</strong>,
                <br>
                <em>Engineering Reports, Wiley</em>, 2021
                <!-- <p>An in‐depth methodical approach to the development of efficient high‐frequency (HF) antennas for use in radio frequency identification (RFID) systems operating at 13.56 MHz.</p> -->
              </td>
            </tr>
          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Teaching</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td width="100%" valign="center">
                <a href="https://cs229s.stanford.edu/fall2023/">Head TA, Systems for Machine Learning (CS 229S), Stanford, Fall 2023</a>
          <br>
          <a href="https://cs229s.stanford.edu/fall2023/">TA, Machine Learning (CS 229), Stanford, Winter 2024</a>
          <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Course Grader: Microprocessors (COE 381) KNUST</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Website template from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
